{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "dcf889db-974c-4522-ab42-afcd146d8162",
                "language": "sql",
                "vscode": {
                    "languageId": "python"
                }
            },
            "outputs": [],
            "source": [
                "from faker import Faker\n",
                "from pymongo import MongoClient\n",
                "import pandas as pd\n",
                "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
                "import json\n",
                "import os\n",
                "\n",
                "# Configuração para o Faker\n",
                "fake = Faker()\n",
                "\n",
                "# Configuração do MongoDB\n",
                "mongo_client = MongoClient(\"mongodb://<username>:<password>@<your_mongo_host>:<your_mongo_port>\")\n",
                "db = mongo_client[\"my_database\"]\n",
                "collection = db[\"fake_data\"]\n",
                "\n",
                "# Configuração do Azure Data Lake\n",
                "AZURE_STORAGE_CONNECTION_STRING = \"DefaultEndpointsProtocol=https;AccountName=<your_account_name>;AccountKey=<your_account_key>;EndpointSuffix=core.windows.net\"\n",
                "CONTAINER_NAME = \"datalake\"\n",
                "BLOB_NAME = \"fake_data/fake_data.json\"\n",
                "\n",
                "# Geração de dados fake\n",
                "def generate_fake_data(num_records=100):\n",
                "    data = []\n",
                "    for _ in range(num_records):\n",
                "        record = {\n",
                "            \"name\": fake.name(),\n",
                "            \"email\": fake.email(),\n",
                "            \"address\": fake.address(),\n",
                "            \"phone_number\": fake.phone_number(),\n",
                "            \"company\": fake.company(),\n",
                "            \"job\": fake.job(),\n",
                "            \"created_at\": fake.date_time_this_year().isoformat(),\n",
                "        }\n",
                "        data.append(record)\n",
                "    return data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "4194b234-c9df-4850-90c9-b88129a46dce",
                "language": "sql",
                "vscode": {
                    "languageId": "python"
                }
            },
            "outputs": [],
            "source": [
                "# Inserção de dados no MongoDB\n",
                "def load_data_to_mongo(data):\n",
                "    collection.insert_many(data)\n",
                "    print(f\"{len(data)} records inserted into MongoDB.\")\n",
                "\n",
                "# Extração e transformação dos dados do MongoDB\n",
                "def extract_and_transform():\n",
                "    data = list(collection.find({}, {\"_id\": 0}))  # Ignorando o campo \"_id\"\n",
                "    df = pd.DataFrame(data)\n",
                "    \n",
                "    # Exemplo de transformação: Adicionar uma nova coluna\n",
                "    df[\"source\"] = \"MongoDB\"\n",
                "    \n",
                "    # Salvando o DataFrame em JSON\n",
                "    transformed_data_path = \"transformed_data.json\"\n",
                "    df.to_json(transformed_data_path, orient=\"records\", lines=True)\n",
                "    return transformed_data_path"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "4e7c893a-6af3-47af-b7e7-ebd03101998e",
                "language": "sql",
                "vscode": {
                    "languageId": "python"
                }
            },
            "outputs": [],
            "source": [
                "# Carregamento dos dados no Data Lake\n",
                "def load_to_datalake(file_path):\n",
                "    blob_service_client = BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING)\n",
                "    blob_client = blob_service_client.get_blob_client(container=CONTAINER_NAME, blob=BLOB_NAME)\n",
                "    \n",
                "    # Criar o container caso ele não exista\n",
                "    try:\n",
                "        container_client = blob_service_client.get_container_client(CONTAINER_NAME)\n",
                "        container_client.create_container()\n",
                "    except Exception as e:\n",
                "        print(\"Container already exists.\")\n",
                "    \n",
                "    # Upload do arquivo JSON\n",
                "    with open(file_path, \"rb\") as data:\n",
                "        blob_client.upload_blob(data, overwrite=True)\n",
                "    print(f\"Data uploaded to Azure Data Lake: {BLOB_NAME}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "ceebb94e-2e85-40e9-aa66-e3ba75b71481",
                "language": "sql",
                "vscode": {
                    "languageId": "python"
                }
            },
            "outputs": [],
            "source": [
                "# Pipeline de ETL completo\n",
                "def etl_pipeline():\n",
                "    # Etapa 1: Gerar dados fake\n",
                "    print(\"Generating fake data...\")\n",
                "    fake_data = generate_fake_data(100)\n",
                "    \n",
                "    # Etapa 2: Inserir no MongoDB\n",
                "    print(\"Loading data into MongoDB...\")\n",
                "    load_data_to_mongo(fake_data)\n",
                "    \n",
                "    # Etapa 3: Extrair e transformar os dados\n",
                "    print(\"Extracting and transforming data...\")\n",
                "    transformed_data_path = extract_and_transform()\n",
                "    \n",
                "    # Etapa 4: Carregar no Azure Data Lake\n",
                "    print(\"Loading data into Azure Data Lake...\")\n",
                "    load_to_datalake(transformed_data_path)\n",
                "    \n",
                "    # Limpar o arquivo local\n",
                "    os.remove(transformed_data_path)\n",
                "    print(\"ETL pipeline completed.\")\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    etl_pipeline()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "SQL",
            "language": "sql",
            "name": "SQL"
        },
        "language_info": {
            "name": "sql",
            "version": ""
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
